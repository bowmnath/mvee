'''
Run MVEE on the document dump data set. The document word counts are stored in
a format similar to COO. Choose n words as our number of dimensions based on
which words are the most or least common.  The number of documents is m. Each
column in a single matrix is the word counts for a single document for some
size-n subset of the words.

generate_matrix and read_data can be used for smaller problems where the entire
data set fits into memory, but they may take up a large amount of disk space
when writing the matrix.

Similar to doc_dumps.py, except that file chooses random subsets of words.
Also similar to ordered_doc_dumps.py, except that file does not work for the
largest document dumps because it requires generating the entire matrix.

Info generated by this file can be displayed with show_ordered_doc_dumps.py.
'''
import pandas as pd
import numpy as np
import numpy.linalg as la
import matplotlib.pyplot as plt
from mvee import mvee2
from mvee import kurtosis
import time
import random
import scipy.linalg as sla



def generate_submatrix(fname, n):
    '''
    Generate a matrix using only the specified rows.
    '''
    print('Generating matrices...')
    rowsums = np.load('data/doc-dumps/orders/%s.npy' % fname)
    with open('data/doc-dumps/docword.%s.txt' % fname, 'r') as f:
        m = int(f.readline())
        mult_factor = 5  # account for rank deficiency

        rows = []
        inds = np.argsort(rowsums)

        # get rid of rows of 0
        first_nonzero = (rowsums == 0).sum()
        medstart = len(inds)//2 - n//2 + first_nonzero
        rows.append(inds[first_nonzero:first_nonzero + n*mult_factor])
        rows.append(inds[medstart:medstart + n])
        rows.append(inds[-n:])
        rows = [list(r) for r in rows]

        mats = []
        for i in range(3):
            mats.append(np.zeros((n, m)))
        mats[0] = np.zeros((n*mult_factor, m))

        f.readline()  # ignore size of dictionary
        total_rows = int(f.readline())
        count = 0
        for line in f:
            count += 1
            if count % 100000 == 0:
                print('Line %d of %d' % (count, total_rows))
            j, i, d = [int(l) for l in line.split()]
            i -= 1
            j -= 1
            for ind in range(n_runs):
                if i in rows[ind]:
                    mats[ind][rows[ind].index(i), j] = d

    count = 0
    small_mat = mats[0][:n]
    while la.matrix_rank(small_mat) < n:
        count += 1
        small_mat = mats[0][count:count + n]
    mats[0] = small_mat

    base_mat_dir = 'data/doc-dumps/matrices/ordered/%s/' % fname
    for i, val in enumerate(['min', 'med', 'max']):
        np.save(base_mat_dir + val, mats[i])

    print('Finished generating matrices.')
    return m, mats


def read_data(fname, n):
    '''
    Note: this does not work for the larger data sets because they cannot
    easily be stored as a dense matrix.
    '''
    print('Reading data...')
    try:
        mats = []
        for val in ['min', 'med', 'max']:
            mats.append(np.load('data/doc-dumps/matrices/ordered/%s/%s.npy' %
                        (fname, val)))
        m = mats[0].shape[1]
    except:
        print('Generating matrix...')
        m, mats = generate_submatrix(fname, n)
        print('Finished generating matrix.')
    print('Data read.')
    return m, mats


do_todd = False
fname = 'pubmed'
epsilon = 1e-6
n = 50
n_runs = 3  # min, med, max

if do_todd:
    added_dir = 'coord-ascent/'
    method = 'todd'
    max_iter = 10000
else:
    added_dir = ''
    method = 'newton'
    max_iter = 1000

base_dir = ('/home/nate/ellipsoids/mvee/outputs/doc-dumps/'
            'ordered/%s%s/' % (added_dir, fname))

m, mats = read_data(fname, n)

try:
    times = np.load(base_dir + 'times.npy')
    cores = np.load(base_dir + 'cores.npy')
    iters = np.load(base_dir + 'iters.npy')
    kurs = np.load(base_dir + 'kurtosis.npy')
    freqs = np.load(base_dir + 'freqs.npy')
    start = np.argmin(times > 0)
except:
    times = np.zeros(n_runs)
    kurs = np.zeros(n_runs)
    cores = np.zeros(n_runs)
    iters = np.zeros(n_runs)
    freqs = np.zeros(n_runs)
    start = 0
for i in range(start, n_runs):
    print('Starting iteration %d of %d' % (i + 1, n_runs))
    X = mats[i]
    kurs[i] = kurtosis(X, do_log=False)
    freqs[i] = np.sum(X)

    try:
        t1 = time.time()
        ret = mvee2(X, initialize='qr', epsilon=epsilon, method=method,
                    verbose=True, max_iter=max_iter, full_output=True,
                    upproject=True, track_count=True)
        t2 = time.time()
        times[i] = t2 - t1
        iters[i] = ret['iter_count']
        cores[i] = (ret['u'] > 1e-12).sum()
        print('total time: ', t2 - t1)
        print('iterations: ', ret['iter_count'])
        print('')
    except Exception as e:
        times[i] = np.inf
        iters[i] = np.inf
        cores[i] = np.inf

    # Data must be saved every iteration because the program tends to get
    # killed when running on larger data
    np.save(base_dir + 'times', times)
    np.save(base_dir + 'cores', cores)
    np.save(base_dir + 'iters', iters)
    np.save(base_dir + 'kurtosis', kurs)
    np.save(base_dir + 'freqs', freqs)

if 1:
    plt.figure()
    plt.plot(freqs, cores, 'o')
    plt.title('Document data (%s)\nm = %d; n = %d' % (fname, m, n))
    plt.xlabel('words in dataset')
    plt.ylabel('core set size')

    plt.figure()
    plt.semilogx(kurs, cores, 'o')
    plt.title('Document data (%s)\nm = %d; n = %d' % (fname, m, n))
    plt.xlabel('kurtosis')
    plt.ylabel('core set size')

    plt.figure()
    plt.semilogy(freqs, kurs, 'o')
    plt.title('Document data (%s)\nm = %d; n = %d' % (fname, m, n))
    plt.xlabel('words in dataset')
    plt.ylabel('kurtosis')

    plt.show()
